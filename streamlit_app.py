import streamlit as st
from openai import OpenAI
from requests_html import HTMLSession
from bs4 import BeautifulSoup


# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

SYSTEM_PROMPT = """
–ü—Ä–æ—Å–∫–æ—Ä—å –∫–∞–Ω–¥–∏–¥–∞—Ç–∞, –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ–Ω –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –¥–∞–Ω–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏.
–°–Ω–∞—á–∞–ª–∞ –Ω–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø–æ—è—Å–Ω—è—Ç—å –æ—Ü–µ–Ω–∫—É.
–û—Ç–¥–µ–ª—å–Ω–æ –æ—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–µ–∑—é–º–µ (–ø–æ–Ω—è—Ç–Ω–æ –ª–∏, —Å –∫–∞–∫–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏ —Å—Ç–∞–ª–∫–∏–≤–∞–ª—Å—è –∫–∞–Ω–¥–∏–¥–∞—Ç –∏ –∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –∏—Ö —Ä–µ—à–∞–ª?). 
–≠—Ç–∞ –æ—Ü–µ–Ω–∫–∞ –¥–æ–ª–∂–Ω–∞ —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –ø—Ä–∏ –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ ‚Äî –Ω–∞–º –≤–∞–∂–Ω–æ –Ω–∞–Ω–∏–º–∞—Ç—å —Ç–∞–∫–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –ø—Ä–æ —Å–≤–æ—é —Ä–∞–±–æ—Ç—É.
–ü–æ—Ç–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –≤–∏–¥–µ –æ—Ü–µ–Ω–∫–∏ –æ—Ç 1 –¥–æ 10.
""".strip()

# --- –ü–ê–†–°–ï–† HH ---
COOKIES = {
    # 'hhuid': '–≤–∞—à_cookie_–µ—Å–ª–∏_–ø–∞—Ä—Å–∏—Ç–µ_—Ä–µ–∑—é–º–µ'
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def get_html(url: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º JS."""
    session = HTMLSession()
    r = session.get(url, headers=HEADERS, cookies=COOKIES)
    try:
        r.html.render(timeout=30, sleep=3)
    except Exception as e:
        print(f"[!] –û—à–∏–±–∫–∞ —Ä–µ–Ω–¥–µ—Ä–∞ JS: {e}")
    return r.html.html


def extract_vacancy_data(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")

    def safe_text(tag, attrs=None):
        el = soup.find(tag, attrs or {})
        return el.get_text(strip=True) if el else "–ù–µ –Ω–∞–π–¥–µ–Ω–æ"

    title = safe_text("h1")
    salary = safe_text("span", {"data-qa": "vacancy-salary"})
    company = safe_text("a", {"data-qa": "vacancy-company-name"})
    description = soup.find("div", {"data-qa": "vacancy-description"})
    description_text = description.get_text(separator="\n").strip() if description else "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

    markdown = f"# {title}\n\n**–ö–æ–º–ø–∞–Ω–∏—è:** {company}\n\n**–ó–∞—Ä–ø–ª–∞—Ç–∞:** {salary}\n\n## –û–ø–∏—Å–∞–Ω–∏–µ\n\n{description_text}"
    return markdown.strip()


def extract_resume_data(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")

    def safe_text(tag, attrs=None):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–≥–∞.
        attrs –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä—ë–º –∞—Ç—Ä–∏–±—É—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä {'data-qa': '...'}) –∏–ª–∏ None.
        """
        if attrs:
            el = soup.find(tag, attrs=attrs)
        else:
            el = soup.find(tag)
        return el.get_text(strip=True) if el else "–ù–µ –Ω–∞–π–¥–µ–Ω–æ"

    name = safe_text("h2", {"data-qa": "resume-personal-name"})
    gender_age = safe_text("p")
    location = safe_text("span", {"data-qa": "resume-personal-address"})
    job_title = safe_text("span", {"data-qa": "resume-block-title-position"})
    job_status = safe_text("span", {"data-qa": "job-search-status"})

    experiences = []
    exp_section = soup.find("div", {"data-qa": "resume-block-experience"})
    if exp_section:
        items = exp_section.find_all("div", class_="resume-block-item-gap")
        for item in items:
            try:
                period_el = item.find("div", class_="bloko-column_s-2")
                period = period_el.get_text(strip=True) if period_el else "–ü–µ—Ä–∏–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω"

                company_el = item.find("div", class_="bloko-text_strong")
                company = company_el.get_text(strip=True) if company_el else "–ö–æ–º–ø–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

                position_el = item.find("div", {"data-qa": "resume-block-experience-position"})
                position = position_el.get_text(strip=True) if position_el else "–î–æ–ª–∂–Ω–æ—Å—Ç—å –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

                desc_el = item.find("div", {"data-qa": "resume-block-experience-description"})
                desc = desc_el.get_text(strip=True) if desc_el else ""

                experiences.append(f"**{period}** ‚Äî *{company}*, {position}\n{desc}")
            except Exception:
                continue

    skills_section = soup.find("div", {"data-qa": "skills-table"})
    skills = []
    if skills_section:
        tags = skills_section.find_all("span", {"data-qa": "bloko-tag__text"})
        skills = [tag.get_text(strip=True) for tag in tags]

    markdown = f"# {name}\n\n**{gender_age}**\n\n**–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** {location}\n\n**–î–æ–ª–∂–Ω–æ—Å—Ç—å:** {job_title}\n\n**–°—Ç–∞—Ç—É—Å:** {job_status}\n\n## –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã\n\n"
    markdown += "\n".join(experiences) if experiences else "–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω.\n"
    markdown += "\n## –ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏\n\n" + (", ".join(skills) if skills else "–ù–∞–≤—ã–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã.\n")
    return markdown.strip()

# --- GPT ---
def request_gpt(system_prompt, user_prompt):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        max_tokens=1000,
        temperature=0,
    )
    return response.choices[0].message.content


# --- Streamlit UI ---
st.title('CV Scoring App')

job_url = st.text_input('–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é —Å hh.ru')
resume_url = st.text_input('–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Ä–µ–∑—é–º–µ —Å hh.ru')

if st.button("–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ"):
    with st.spinner("–ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ GPT..."):
        try:
            # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü
            job_html = get_html(job_url)
            resume_html = get_html(resume_url)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            job_text = extract_vacancy_data(job_html)
            resume_text = extract_resume_data(resume_html)

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = f"# –í–ê–ö–ê–ù–°–ò–Ø\n{job_text}\n\n# –†–ï–ó–Æ–ú–ï\n{resume_text}"

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ GPT
            response = request_gpt(SYSTEM_PROMPT, prompt)

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:")
            st.markdown(response)

        except Exception as e:
            st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
